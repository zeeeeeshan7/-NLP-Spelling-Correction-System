{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d66c1a4-a033-4191-ad98-c8cf5a00c4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   abstract_id        line_id  \\\n",
      "0     24491034  24491034_0_11   \n",
      "1     24491034  24491034_1_11   \n",
      "2     24491034  24491034_2_11   \n",
      "3     24491034  24491034_3_11   \n",
      "4     24491034  24491034_4_11   \n",
      "\n",
      "                                       abstract_text  line_number  \\\n",
      "0  The emergence of HIV as a chronic condition me...            0   \n",
      "1  This paper describes the design and evaluation...            1   \n",
      "2  This study is designed as a randomised control...            2   \n",
      "3  The intervention group will participate in the...            3   \n",
      "4  The program is based on self-efficacy theory a...            4   \n",
      "\n",
      "   total_lines      target  \n",
      "0           11  BACKGROUND  \n",
      "1           11  BACKGROUND  \n",
      "2           11     METHODS  \n",
      "3           11     METHODS  \n",
      "4           11     METHODS  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2211861 entries, 0 to 2211860\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Dtype \n",
      "---  ------         ----- \n",
      " 0   abstract_id    int64 \n",
      " 1   line_id        object\n",
      " 2   abstract_text  object\n",
      " 3   line_number    int64 \n",
      " 4   total_lines    int64 \n",
      " 5   target         object\n",
      "dtypes: int64(3), object(3)\n",
      "memory usage: 101.3+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#Import libraries and load the CSV\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from math import log\n",
    "import math\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "from numpy.linalg import norm\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"NLP_train.csv\")\n",
    "\n",
    "# Displaying the first rows\n",
    "print(df.head())\n",
    "print(df.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b612d7ee-70ab-4fff-8181-729af47097c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50183758,\n",
       " ['the',\n",
       "  'emergence',\n",
       "  'of',\n",
       "  'hiv',\n",
       "  'as',\n",
       "  'a',\n",
       "  'chronic',\n",
       "  'condition',\n",
       "  'means',\n",
       "  'that',\n",
       "  'people',\n",
       "  'living',\n",
       "  'with',\n",
       "  'hiv',\n",
       "  'are',\n",
       "  'required',\n",
       "  'to',\n",
       "  'take',\n",
       "  'more',\n",
       "  'responsibility'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Build the medical corpus\n",
    "# join all abstract_text into one big corpus\n",
    "texts = df[\"abstract_text\"].astype(str).tolist()\n",
    "corpus = \" \".join(texts)\n",
    "\n",
    "# tokenize: keep letters and '@' (numbers replaced in this dataset)\n",
    "TOK_RE = re.compile(r\"[A-Za-z@]+\")\n",
    "tokens = TOK_RE.findall(corpus.lower())\n",
    "\n",
    "len(tokens), tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6122df4c-9aa9-43f1-a5e6-a34c1092dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#More than 50 million tokens in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d19fc22-887a-4ac1-b497-665257f4dc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build Unigram and Bigram Models\n",
    "uni = Counter(tokens)\n",
    "bigrams = list(zip(tokens[:-1], tokens[1:]))\n",
    "bi = Counter(bigrams)\n",
    "\n",
    "# totals\n",
    "N_tokens = sum(uni.values())\n",
    "V = len(uni)\n",
    "\n",
    "# Probability Smoothing\n",
    "def p_uni(w, k=1.0):\n",
    "    return (uni.get(w, 0) + k) / (N_tokens + k * V)\n",
    "\n",
    "def p_bi(w1, w2, k=1.0):\n",
    "    return (bi.get((w1, w2), 0) + k) / (uni.get(w1, 0) + k * V)\n",
    "\n",
    "# vocab set\n",
    "VOCAB = set(uni.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bbeeed4-cdfa-4ace-adb7-fd57ea791cdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('@', 3750167), ('the', 2173891), ('of', 1715701), ('and', 1670719), ('in', 1345241), ('to', 939291), ('with', 755421), ('a', 750779), ('were', 653116), ('was', 595127), ('patients', 524715), ('for', 488957), ('group', 427238), ('p', 359232), ('or', 338495), ('at', 293007), ('treatment', 247640), ('on', 237590), ('study', 233969), ('after', 213605)]\n"
     ]
    }
   ],
   "source": [
    "# full, frequency-sorted list (descending)\n",
    "sorted_vocab = sorted(uni.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "def search_vocab(q, top=30):\n",
    "    q = q.lower()\n",
    "    return [(w, c) for (w, c) in sorted_vocab if q in w][:top]\n",
    "\n",
    "# examples\n",
    "sorted_vocab[:20]           \n",
    "print(sorted_vocab[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "13f9af85-a13d-4b07-8658-4f5df86d2c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Candidate Generation with Minimum Edit Distance\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz@\"\n",
    "\n",
    "def edits1(word):\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word)+1)]\n",
    "    deletes    = [L + R[1:]           for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    replaces   = [L + c + R[1:]       for L, R in splits if R for c in alphabet]\n",
    "    inserts    = [L + c + R           for L, R in splits for c in alphabet]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def known(words):\n",
    "    \"\"\"Return only words that are actually in the vocabulary.\"\"\"\n",
    "    return [w for w in words if w in VOCAB]\n",
    "\n",
    "def candidates(word, max_edits=2):\n",
    "    \"\"\"Generate spelling correction candidates for a word.\"\"\"\n",
    "    cands = set()\n",
    "    e1 = edits1(word)\n",
    "    cands.update(known(e1))\n",
    "    if max_edits >= 2:\n",
    "        for w in list(e1)[:5000]: \n",
    "            cands.update(known(edits1(w)))\n",
    "    if word in VOCAB:\n",
    "        cands.add(word)\n",
    "    return list(cands)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "058e5009-95fb-41bc-b691-3479fa832a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop @ from suggestions\n",
    "\n",
    "USE_AT = False \n",
    "if not USE_AT and '@' in VOCAB:\n",
    "    VOCAB.discard('@')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46efcda7-42dc-402a-aa0a-9a462c561458",
   "metadata": {},
   "outputs": [],
   "source": [
    "#La Place Smoothing for Candidate Words\n",
    "\n",
    "def p_uni_smoothed(w, k=1.0):\n",
    "    return (uni.get(w, 0) + k) / (N_tokens + k * len(uni))\n",
    "\n",
    "def p_bi_smoothed(w1, w2, k=1.0):\n",
    "    return (bi.get((w1, w2), 0) + k) / (uni.get(w1, 0) + k * len(uni))\n",
    "\n",
    "def logscore_context(left, w, right, k=1.0, lam=0.7):\n",
    "    \"\"\"\n",
    "    Interpolate bigram(s) with unigram:\n",
    "      score = lam*(log P(w|left) + log P(right|w)) + (1-lam)*log P(w)\n",
    "    If left/right missing, those terms drop out.\n",
    "    \"\"\"\n",
    "    s = 0.0\n",
    "    used = 0\n",
    "    if left:\n",
    "        s += log(p_bi_smoothed(left, w, k)); used += 1\n",
    "    if right:\n",
    "        s += log(p_bi_smoothed(w, right, k)); used += 1\n",
    "    if used:\n",
    "        return lam * s + (1 - lam) * log(p_uni_smoothed(w, k))\n",
    "    else:\n",
    "        return log(p_uni_smoothed(w, k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f1f3896-7d12-4b62-81b0-bf8efa1cd7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Damerau-Levenshtein distance between the original word and the candidate word\n",
    "def approx_edit_distance(a, b):\n",
    "    # True Damerauâ€“Levenshtein: insertions, deletions, substitutions, transpositions\n",
    "    return nltk.edit_distance(a, b, transpositions=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e13fca7-0f09-4677-8d0f-2c481993c449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#User Input Tokenization\n",
    "\n",
    "TOK_RE = re.compile(r\"[A-Za-z@]+\")\n",
    "\n",
    "def tokenize_user(text):\n",
    "    return TOK_RE.findall(text.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c089f8c-785e-445d-9300-3908bd0c99b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def suggest_for_token(tokens, idx, topk=5, k=1.0, lam=0.7, edit_penalty=0.75):\n",
    "    \"\"\"\n",
    "    Simple baseline suggestion generator (medical corpus only).\n",
    "    No POS, no IDF, no normalization.\n",
    "    \"\"\"\n",
    "    w = tokens[idx].lower()\n",
    "    left  = tokens[idx-1].lower() if idx-1 >= 0 else None\n",
    "    right = tokens[idx+1].lower() if idx+1 < len(tokens) else None\n",
    "\n",
    "    cand_list = candidates(w, max_edits=2)\n",
    "    if not cand_list:\n",
    "        return []\n",
    "\n",
    "    ranked = []\n",
    "    for c in cand_list:\n",
    "        ed = approx_edit_distance(w, c)\n",
    "        # force numeric conversion to avoid str issues\n",
    "        raw_score = logscore_context(left, c, right, k=k, lam=lam)\n",
    "        score = float(raw_score) - edit_penalty * ed\n",
    "        ranked.append({\n",
    "            \"cand\": c,\n",
    "            \"score\": round(score, 3),\n",
    "            \"edit\": ed\n",
    "        })\n",
    "\n",
    "    ranked.sort(key=lambda x: (-x[\"score\"], x[\"edit\"], x[\"cand\"]))\n",
    "    return ranked[:topk]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f40e8d4d-f1e7-4b8b-b65b-9bc763ea989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_suggest(text, topk=5, ctx_margin=1.0):\n",
    "    toks = tokenize_user(text)\n",
    "    results = []\n",
    "\n",
    "    for i, w in enumerate(toks):\n",
    "        w = w.lower()\n",
    "        if w == '@':   # skip placeholder tokens\n",
    "            continue\n",
    "\n",
    "        in_vocab = w in VOCAB\n",
    "        suggestions = suggest_for_token(toks, i, topk=topk)\n",
    "\n",
    "        # If it's a non-word: show suggestions if any\n",
    "        if not in_vocab and suggestions:\n",
    "            results.append({\n",
    "                \"index\": i,\n",
    "                \"word\": w,\n",
    "                \"type\": \"NON-WORD\",\n",
    "                \"suggestions\": suggestions   # already dicts\n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # If it's a real word: only flag if best alternative beats the original by ctx_margin\n",
    "        if in_vocab and suggestions:\n",
    "            left  = toks[i-1].lower() if i-1 >= 0 else None\n",
    "            right = toks[i+1].lower() if i+1 < len(toks) else None\n",
    "            orig_score = float(logscore_context(left, w, right))\n",
    "\n",
    "            best_cand = suggestions[0][\"cand\"]\n",
    "            best_score = suggestions[0][\"score\"]\n",
    "            best_ed = suggestions[0][\"edit\"]\n",
    "\n",
    "            if best_cand != w and (best_score - orig_score) >= ctx_margin:\n",
    "                results.append({\n",
    "                    \"index\": i,\n",
    "                    \"word\": w,\n",
    "                    \"type\": \"REAL-WORD?\",\n",
    "                    \"orig_score\": round(orig_score, 3),\n",
    "                    \"best_delta\": round(best_score - orig_score, 3),\n",
    "                    \"suggestions\": suggestions   # already dicts\n",
    "                })\n",
    "\n",
    "    return toks, results\n",
    "\n",
    "                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "870ea318-892f-4ca7-80cd-de99fe44e548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INPUT: The patints were given aspirin daily.\n",
      "TOKENS: ['the', 'patints', 'were', 'given', 'aspirin', 'daily']\n",
      "{'index': 1, 'word': 'patints', 'type': 'REAL-WORD?', 'orig_score': -23.673, 'best_delta': 16.593, 'suggestions': [{'cand': 'patients', 'score': -7.08, 'edit': 1}, {'cand': 'patient', 'score': -12.032, 'edit': 2}, {'cand': 'points', 'score': -14.245, 'edit': 2}, {'cand': 'parents', 'score': -14.328, 'edit': 2}, {'cand': 'ratings', 'score': -15.768, 'edit': 2}]}\n",
      "\n",
      "INPUT: The study focused on diabtes treatment.\n",
      "TOKENS: ['the', 'study', 'focused', 'on', 'diabtes', 'treatment']\n",
      "{'index': 4, 'word': 'diabtes', 'type': 'NON-WORD', 'suggestions': [{'cand': 'diabetes', 'score': -13.741, 'edit': 1}, {'cand': 'diaries', 'score': -20.89, 'edit': 2}, {'cand': 'dates', 'score': -22.662, 'edit': 2}, {'cand': 'diabtel', 'score': -22.882, 'edit': 1}, {'cand': 'debates', 'score': -23.248, 'edit': 2}]}\n",
      "\n",
      "INPUT: He suffered from a hearth attack.\n",
      "TOKENS: ['he', 'suffered', 'from', 'a', 'hearth', 'attack']\n",
      "{'index': 0, 'word': 'he', 'type': 'REAL-WORD?', 'orig_score': -11.143, 'best_delta': 2.43, 'suggestions': [{'cand': 'who', 'score': -8.713, 'edit': 2}, {'cand': 'had', 'score': -9.07, 'edit': 2}, {'cand': 'have', 'score': -9.702, 'edit': 2}, {'cand': 'or', 'score': -10.362, 'edit': 2}, {'cand': 'b', 'score': -10.665, 'edit': 2}]}\n",
      "{'index': 4, 'word': 'hearth', 'type': 'REAL-WORD?', 'orig_score': -22.731, 'best_delta': 8.645, 'suggestions': [{'cand': 'heart', 'score': -14.086, 'edit': 1}, {'cand': 'health', 'score': -16.241, 'edit': 1}, {'cand': 'healthy', 'score': -17.504, 'edit': 2}, {'cand': 'heat', 'score': -19.692, 'edit': 2}, {'cand': 'death', 'score': -19.743, 'edit': 2}]}\n",
      "\n",
      "INPUT: The drug aspitin was administered.\n",
      "TOKENS: ['the', 'drug', 'aspitin', 'was', 'administered']\n",
      "{'index': 2, 'word': 'aspitin', 'type': 'NON-WORD', 'suggestions': [{'cand': 'aspirin', 'score': -16.913, 'edit': 1}, {'cand': 'ascitic', 'score': -22.35, 'edit': 2}, {'cand': 'asprin', 'score': -22.53, 'edit': 2}, {'cand': 'aspinin', 'score': -22.594, 'edit': 1}, {'cand': 'respitin', 'score': -22.969, 'edit': 2}]}\n"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    \"The patints were given aspirin daily.\",      # patients\n",
    "    \"The study focused on diabtes treatment.\",    # diabetes\n",
    "    \"He suffered from a hearth attack.\",          # heart\n",
    "    \"The drug aspitin was administered.\",         # aspirin\n",
    "]\n",
    "\n",
    "for t in tests:\n",
    "    toks, res = detect_and_suggest(t, topk=5, ctx_margin=0.8) #more lenient ctx_margin\n",
    "    print(\"\\nINPUT:\", t)\n",
    "    print(\"TOKENS:\", toks)\n",
    "    for r in res:\n",
    "        print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb161b23-f4d6-4594-979b-c1e16274143d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_corrections(text, results, ctx_only=True):\n",
    "    \"\"\"\n",
    "    ctx_only=True: for REAL-WORD? apply only when flagged (i.e., delta >= margin)\n",
    "    \"\"\"\n",
    "    toks = tokenize_user(text)\n",
    "    idx_to_best = {}\n",
    "    for r in results:\n",
    "        if not r.get(\"suggestions\"):\n",
    "            continue\n",
    "        best = r[\"suggestions\"][0][\"cand\"]\n",
    "        if r[\"type\"] == \"NON-WORD\":\n",
    "            idx_to_best[r[\"index\"]] = best\n",
    "        elif r[\"type\"] == \"REAL-WORD?\":\n",
    "            # already passed the margin filter in detect_and_suggest\n",
    "            idx_to_best[r[\"index\"]] = best\n",
    "\n",
    "    corrected = [ (idx_to_best[i] if i in idx_to_best else t) for i, t in enumerate(toks) ]\n",
    "    return \" \".join(corrected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "768ca1fa-491f-4824-b8fb-0fb313225a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOKENS: ['the', 'patints', 'suffered', 'a', 'hearth', 'attack']\n",
      "{'index': 1, 'word': 'patints', 'type': 'REAL-WORD?', 'orig_score': -23.673, 'best_delta': 12.572, 'suggestions': [{'cand': 'patients', 'score': -11.101, 'edit': 1}, {'cand': 'patient', 'score': -13.414, 'edit': 2}, {'cand': 'parents', 'score': -18.068, 'edit': 2}, {'cand': 'points', 'score': -19.623, 'edit': 2}, {'cand': 'ratings', 'score': -19.975, 'edit': 2}]}\n",
      "{'index': 4, 'word': 'hearth', 'type': 'REAL-WORD?', 'orig_score': -22.731, 'best_delta': 8.645, 'suggestions': [{'cand': 'heart', 'score': -14.086, 'edit': 1}, {'cand': 'health', 'score': -16.241, 'edit': 1}, {'cand': 'healthy', 'score': -17.504, 'edit': 2}, {'cand': 'heat', 'score': -19.692, 'edit': 2}, {'cand': 'death', 'score': -19.743, 'edit': 2}]}\n",
      "\n",
      "Corrected: the patients suffered a heart attack\n"
     ]
    }
   ],
   "source": [
    "#Testing detect_and_suggest for the medical corpus\n",
    "text = \"The patints suffered a hearth attack.\"\n",
    "toks, res = detect_and_suggest(text)\n",
    "print(\"TOKENS:\", toks)\n",
    "for r in res:\n",
    "    print(r)\n",
    "\n",
    "corrected = apply_corrections(text, res)\n",
    "print(\"\\nCorrected:\", corrected)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8955fb82-f47b-4c9e-995f-72670826d566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "General corpus size: 9076\n",
      "Sample: ['the', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', 'my', 'mum', 'tries', 'to', 'be', 'cool', 'by', 'saying', 'that', 'she', 'likes']\n"
     ]
    }
   ],
   "source": [
    "#Adding a general English corpus to boost the performance of the spelling correction (Loading and tokenization of general English corpus)\n",
    "\n",
    "# Load the general English corpus\n",
    "with open(\"sentences.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    general_text = f.read()\n",
    "\n",
    "# Tokenize: keep only letters\n",
    "general_tokens = TOK_RE.findall(general_text.lower())\n",
    "print(\"General corpus size:\", len(general_tokens))\n",
    "print(\"Sample:\", general_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59e6d89b-686c-4c51-a3c6-1ce54c88d269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique words in general corpus: 2563\n"
     ]
    }
   ],
   "source": [
    "#Building unigram and bigram counts for general corpus\n",
    "\n",
    "uni_general = Counter(general_tokens)\n",
    "bi_general = Counter(zip(general_tokens[:-1], general_tokens[1:]))\n",
    "\n",
    "print(\"Unique words in general corpus:\", len(uni_general))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f860e6b-0faf-4574-a610-0d305d96bca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Building the hyprid corpus\n",
    "\n",
    "# Merge unigram counts\n",
    "uni_hybrid = uni + uni_general   # uni = PubMed unigrams  uni_general = General English unigrams\n",
    "\n",
    "# Merge bigram counts\n",
    "bi_hybrid = bi + bi_general      # bi = your PubMed bigrams bi_general = General English bigrams\n",
    "\n",
    "# Hybrid totals\n",
    "N_tokens_hybrid = sum(uni_hybrid.values())\n",
    "V_hybrid = len(uni_hybrid)\n",
    "\n",
    "# Hybrid vocab\n",
    "VOCAB_HYBRID = set(uni_hybrid.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4e16824b-f323-4b22-8920-ddf81db58fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Semantic Context Scoring\n",
    "def get_context_vector(tokens, idx, window=2):\n",
    "    ctx_words = []\n",
    "    for j in range(max(0, idx-window), min(len(tokens), idx+window+1)):\n",
    "        if j != idx and tokens[j] in word_vectors:\n",
    "            ctx_words.append(word_vectors[tokens[j]])\n",
    "    if ctx_words:\n",
    "        return np.mean(ctx_words, axis=0)\n",
    "    return None\n",
    "\n",
    "def semantic_similarity(candidate, context_vec):\n",
    "    if candidate in word_vectors and context_vec is not None:\n",
    "        cand_vec = word_vectors[candidate]\n",
    "        return float(np.dot(cand_vec, context_vec) / (norm(cand_vec) * norm(context_vec)))\n",
    "    return 0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "54fc286a-cf35-4d0d-8c62-e772a19153a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out words from the hybrid corpus that appear less than 15 times\n",
    "\n",
    "token_counts = Counter(tokens)\n",
    "\n",
    "# filter out very rare words (likely misspellings)\n",
    "VOCAB_HYBRID = {w for w, c in token_counts.items() if c >= 15}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7a329638-76c2-49bb-8602-cbec4acadf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs counted: 2212585\n",
      "Sample df: [('responsibility', 228), ('to', 733327), ('their', 40928), ('take', 2127), ('with', 606982), ('a', 597165), ('for', 407227), ('the', 1264429), ('self', 24094), ('management', 18978)]\n"
     ]
    }
   ],
   "source": [
    "# Build hybrid \"documents\"\n",
    "# Use PubMed abstracts + general English sentences\n",
    "pubmed_docs = df[\"abstract_text\"].astype(str).tolist()\n",
    "general_docs = general_text.split(\"\\n\")\n",
    "\n",
    "docs = pubmed_docs + general_docs  \n",
    "\n",
    "# Count document frequencies\n",
    "doc_freqs = Counter()\n",
    "for doc in docs:\n",
    "    unique_words = set(TOK_RE.findall(doc.lower()))\n",
    "    doc_freqs.update(unique_words)\n",
    "\n",
    "N_docs = len(docs)\n",
    "print(\"Docs counted:\", N_docs)\n",
    "print(\"Sample df:\", list(doc_freqs.items())[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5207a7f1-87f4-4b56-8dab-d6fda85c1d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Updating probability functions\n",
    "\n",
    "def p_uni_hybrid(w, k=1.0):\n",
    "    return (uni_hybrid.get(w, 0) + k) / (N_tokens_hybrid + k * V_hybrid)\n",
    "\n",
    "def p_bi_hybrid(w1, w2, k=1.0):\n",
    "    return (bi_hybrid.get((w1, w2), 0) + k) / (uni_hybrid.get(w1, 0) + k * V_hybrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "25a68f58-c5f2-436c-8b99-29432326e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hybrid vocab browsing\n",
    "sorted_vocab_hybrid = sorted(uni_hybrid.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "def search_vocab_hybrid(q, top=30):\n",
    "    q = q.lower()\n",
    "    return [(w, c) for (w, c) in sorted_vocab_hybrid if q in w][:top]\n",
    "\n",
    "# --- Aliases so rest of code uses hybrid by default\n",
    "VOCAB = VOCAB_HYBRID\n",
    "p_uni = p_uni_hybrid\n",
    "p_bi = p_bi_hybrid\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea0cd869-6985-4ebd-bf93-43967faae8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inverse Document Frequency\n",
    "\n",
    "def idf_score(word, doc_freqs, N_docs):\n",
    "    \"\"\"Inverse document frequency: penalize very common words.\"\"\"\n",
    "    df = doc_freqs.get(word, 1)\n",
    "    return math.log((N_docs + 1) / (df + 1)) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a40ea8d9-4a55-44cb-b67f-71aa95133af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cosine Similarity for Contextuality\n",
    "def semantic_similarity(w, ctx_vec):\n",
    "    \"\"\"\n",
    "    Cosine similarity between a candidate word and the context vector.\n",
    "    Returns a score in [-1, 1], usually 0â€“1 for embeddings.\n",
    "    \"\"\"\n",
    "    if w not in word_vectors or ctx_vec is None:\n",
    "        return 0.0\n",
    "    v = word_vectors[w]\n",
    "    return float(np.dot(v, ctx_vec) / (norm(v) * norm(ctx_vec)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ae099de6-ef85-4e29-a20b-026d7694156f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/elenawachter/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#POS-Tagging\n",
    "\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "def same_pos(word1, word2):\n",
    "    \"\"\"\n",
    "    Return True if word1 and word2 share the same POS tag.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        pos1 = nltk.pos_tag([word1])[0][1]\n",
    "        pos2 = nltk.pos_tag([word2])[0][1]\n",
    "        return pos1 == pos2\n",
    "    except:\n",
    "        return True  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c04d1691-9db8-454b-847c-56795b33ac66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yl/gr6zr76s6xb297ltlygh3wf40000gn/T/ipykernel_25536/3627136711.py:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
     ]
    }
   ],
   "source": [
    "#Preparation and Loading of Pre-Trained Word Embeddings\n",
    "glove_input_file = \"glove.6B.50d.txt\"\n",
    "word2vec_output_file = \"glove.6B.50d.word2vec.txt\"\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)\n",
    "\n",
    "word_vectors = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7a1f9b2f-1c30-42f8-b3e6-e5979cb5887f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Suggest for token for hybrid\n",
    "\n",
    "def suggest_for_token(tokens, idx, topk=5, k=1.0, lam=0.7, edit_penalty=0.75):\n",
    "    w = tokens[idx].lower()\n",
    "    left  = tokens[idx-1].lower() if idx-1 >= 0 else None\n",
    "    right = tokens[idx+1].lower() if idx+1 < len(tokens) else None\n",
    "\n",
    "    cand_list = candidates(w, max_edits=2)\n",
    "    if not cand_list:\n",
    "        return []\n",
    "\n",
    "    ranked = []\n",
    "    for c in cand_list:\n",
    "        ed = approx_edit_distance(w, c)\n",
    "        raw_score = logscore_context(left, c, right, k=k, lam=lam)\n",
    "        score = float(raw_score) - edit_penalty * ed\n",
    "        ranked.append({\n",
    "            \"cand\": c,\n",
    "            \"score\": round(score, 3),\n",
    "            \"edit\": ed\n",
    "        })\n",
    "\n",
    "    ranked.sort(key=lambda x: (-x[\"score\"], x[\"edit\"], x[\"cand\"]))\n",
    "    return ranked[:topk]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c713bf85-92a5-458c-9869-58a89318bbf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect and Suggest Hybrid\n",
    "def detect_and_suggest_hybrid(text, topk=5, ctx_margin=0.8, k=3, lam=0.8, edit_penalty=0.5):\n",
    "    toks = tokenize_user(text)\n",
    "    results = []\n",
    "\n",
    "    for i, w in enumerate(toks):\n",
    "        w_norm = w.lower()\n",
    "        if w_norm == '@':   # skip placeholder tokens\n",
    "            continue\n",
    "\n",
    "        in_vocab = w_norm in VOCAB_HYBRID\n",
    "        suggestions = suggest_for_token(toks, i, topk=topk, k=k, lam=lam, edit_penalty=edit_penalty)\n",
    "\n",
    "        # --- NON-WORD: always flag, even if no suggestions ---\n",
    "        if not in_vocab:\n",
    "            results.append({\n",
    "                \"index\": i,\n",
    "                \"word\": w_norm,\n",
    "                \"type\": \"NON-WORD\",\n",
    "                \"suggestions\": suggestions  \n",
    "            })\n",
    "            continue\n",
    "\n",
    "        # --- REAL-WORD: only flag if best candidate clearly better ---\n",
    "        if in_vocab and suggestions:\n",
    "            left = toks[i-1].lower() if i-1 >= 0 else None\n",
    "            right = toks[i+1].lower() if i+1 < len(toks) else None\n",
    "            orig_score = float(logscore_context(left, w_norm, right, k=k, lam=lam))\n",
    "\n",
    "            best_cand = suggestions[0][\"cand\"]\n",
    "            best_score = suggestions[0][\"score\"]\n",
    "            best_ed = suggestions[0][\"edit\"]\n",
    "\n",
    "            MAX_REALWORD_EDIT = 1\n",
    "            if (best_ed <= MAX_REALWORD_EDIT\n",
    "                and best_cand != w_norm\n",
    "                and (best_score - orig_score) >= ctx_margin\n",
    "            ):\n",
    "                results.append({\n",
    "                    \"index\": i,\n",
    "                    \"word\": w_norm,\n",
    "                    \"type\": \"REAL-WORD\",\n",
    "                    \"orig_score\": round(orig_score, 3),\n",
    "                    \"best_delta\": round(best_score - orig_score, 3),\n",
    "                    \"suggestions\": suggestions\n",
    "                })\n",
    "\n",
    "    return toks, results\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c4fc2a4b-cefa-4ca9-963f-d36682798909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For hybrid 'Apply Correction'\n",
    "def apply_corrections_hybrid(text, results, ctx_only=True):\n",
    "    \"\"\"\n",
    "    ctx_only=True: for REAL-WORD? apply only when flagged (i.e., delta >= margin)\n",
    "    \"\"\"\n",
    "    toks = tokenize_user(text)\n",
    "    idx_to_best = {}\n",
    "    for r in results:\n",
    "        if not r.get(\"suggestions\"):\n",
    "            continue\n",
    "        best = r[\"suggestions\"][0][\"cand\"]\n",
    "        if r[\"type\"] == \"NON-WORD\":\n",
    "            idx_to_best[r[\"index\"]] = best\n",
    "        elif r[\"type\"] == \"REAL-WORD?\":\n",
    "            # already passed the margin filter in detect_and_suggest_hybrid\n",
    "            idx_to_best[r[\"index\"]] = best\n",
    "\n",
    "    corrected = [ (idx_to_best[i] if i in idx_to_best else t) for i, t in enumerate(toks) ]\n",
    "    return \" \".join(corrected)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5598437d-a782-46b5-9660-18363a2b79ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "INPUT: The patints were given aspitin to reduce pain.\n",
      "TOKENS: ['the', 'patints', 'were', 'given', 'aspitin', 'to', 'reduce', 'pain']\n",
      " -> Word: patints (NON-WORD)\n",
      "    cand=patients, score=-7.459, edit=1\n",
      "    cand=patient, score=-12.865, edit=2\n",
      "    cand=parents, score=-15.299, edit=2\n",
      "    cand=points, score=-15.31, edit=2\n",
      "    cand=ratings, score=-16.886, edit=2\n",
      " -> Word: aspitin (NON-WORD)\n",
      "    cand=aspirin, score=-17.063, edit=1\n",
      "    cand=ascitic, score=-22.759, edit=2\n",
      "\n",
      "INPUT: High blodsugar levels are dangerous for diabtes patients.\n",
      "TOKENS: ['high', 'blodsugar', 'levels', 'are', 'dangerous', 'for', 'diabtes', 'patients']\n",
      " -> Word: blodsugar (NON-WORD)\n",
      " -> Word: diabtes (NON-WORD)\n",
      "    cand=diabetes, score=-13.752, edit=1\n",
      "    cand=diaries, score=-21.628, edit=2\n",
      "    cand=dates, score=-22.527, edit=2\n",
      "    cand=debates, score=-23.539, edit=2\n",
      "\n",
      "INPUT: The doctor monitored the insuline dosage carefully.\n",
      "TOKENS: ['the', 'doctor', 'monitored', 'the', 'insuline', 'dosage', 'carefully']\n",
      " -> Word: insuline (NON-WORD)\n",
      "    cand=insulin, score=-15.152, edit=1\n",
      "    cand=inulin, score=-22.478, edit=2\n",
      "    cand=insulins, score=-22.739, edit=1\n",
      "    cand=inline, score=-23.496, edit=2\n",
      "    cand=incline, score=-24.126, edit=2\n",
      "\n",
      "INPUT: He sufferd an atack yesterday.\n",
      "TOKENS: ['he', 'sufferd', 'an', 'atack', 'yesterday']\n",
      " -> Word: sufferd (NON-WORD)\n",
      "    cand=suffered, score=-19.433, edit=1\n",
      "    cand=suffer, score=-21.218, edit=1\n",
      "    cand=suffers, score=-22.153, edit=1\n",
      "    cand=buffer, score=-22.223, edit=2\n",
      "    cand=buffered, score=-22.289, edit=2\n",
      " -> Word: atack (NON-WORD)\n",
      "    cand=attack, score=-19.822, edit=1\n",
      "    cand=back, score=-22.02, edit=2\n",
      "    cand=task, score=-22.067, edit=2\n",
      "    cand=lack, score=-22.105, edit=2\n",
      "    cand=black, score=-22.249, edit=2\n",
      " -> Word: yesterday (NON-WORD)\n"
     ]
    }
   ],
   "source": [
    "tests = [\n",
    "    \"The patints were given aspitin to reduce pain.\",\n",
    "    \"High blodsugar levels are dangerous for diabtes patients.\",\n",
    "    \"The doctor monitored the insuline dosage carefully.\",\n",
    "    \"He sufferd an atack yesterday.\"\n",
    "]\n",
    "\n",
    "\n",
    "for t in tests:\n",
    "    toks, res = detect_and_suggest_hybrid(t, topk=5, ctx_margin=0.8)\n",
    "    print(\"\\nINPUT:\", t)\n",
    "    print(\"TOKENS:\", toks)\n",
    "\n",
    "    for r in res:  # r is a dictionary now\n",
    "        print(f\" -> Word: {r['word']} ({r['type']})\")\n",
    "        for s in r[\"suggestions\"]:\n",
    "            print(f\"    cand={s['cand']}, score={s['score']}, edit={s['edit']}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de56953-5bb2-4612-9171-5b5a96f65209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b410ddc-8254-4216-a267-dec284f603aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "899c548d-f277-4224-8dda-a56a4c021b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "%gui tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b02c71-e87b-428e-98ed-d2e2db65c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "\n",
    "MAX_CHARS = 500\n",
    "\n",
    "# Regex for tokenization\n",
    "try:\n",
    "    TOK_RE\n",
    "except NameError:\n",
    "    TOK_RE = re.compile(r\"[A-Za-z@]+\")\n",
    "\n",
    "# --- tokenize with spans (for highlighting & click mapping)\n",
    "def tokenize_with_spans(text):\n",
    "    spans = []\n",
    "    for m in TOK_RE.finditer(text.lower()):\n",
    "        spans.append((m.group(0), m.start(), m.end()))\n",
    "    return spans\n",
    "\n",
    "class SpellApp(tk.Tk):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.title(\"MedSpell Studio â€” Hybrid Corpus\")\n",
    "        self.geometry(\"980x620\")\n",
    "        self.configure(bg=\"#3E2723\")  # dark brown background\n",
    "        self._build_style()\n",
    "        self._build_layout()\n",
    "        self._bind_keys()\n",
    "\n",
    "        # state\n",
    "        self.spans = []\n",
    "        self.tokens = []\n",
    "        self.detect_results = []\n",
    "        self.issue_indices = []\n",
    "        self.issue_cursor = 0\n",
    "\n",
    "        # demo text (changed here)\n",
    "        demo = \"The doctor prescribed insuline for the patient.\"\n",
    "        self.txt.insert(\"1.0\", demo)\n",
    "        self._enforce_limit()\n",
    "        self.run_check()\n",
    "\n",
    "    # ---------- UI ----------\n",
    "    def _build_style(self):\n",
    "        style = ttk.Style(self)\n",
    "        style.theme_use(\"clam\")\n",
    "        style.configure(\".\", background=\"#3E2723\", foreground=\"#FFF3E0\")\n",
    "        style.configure(\"TButton\", padding=6, background=\"#5D4037\", foreground=\"#FFF3E0\")\n",
    "        style.map(\"TButton\", background=[(\"active\", \"#6D4C41\")])\n",
    "        style.configure(\"TLabel\", background=\"#3E2723\", foreground=\"#FFF3E0\")\n",
    "        style.configure(\"TFrame\", background=\"#3E2723\")\n",
    "        style.configure(\"Treeview\", background=\"#4E342E\", fieldbackground=\"#4E342E\",\n",
    "                        foreground=\"#FFF3E0\", rowheight=24)\n",
    "        style.configure(\"Treeview.Heading\", background=\"#5D4037\", foreground=\"#FFF3E0\")\n",
    "\n",
    "    def _build_layout(self):\n",
    "        # Top toolbar\n",
    "        bar = ttk.Frame(self); bar.pack(fill=\"x\", padx=10, pady=10)\n",
    "        ttk.Button(bar, text=\"Check (Ctrl/Cmd+Enter)\", command=self.run_check).pack(side=\"left\", padx=4)\n",
    "        ttk.Button(bar, text=\"Auto-correct Top\", command=self.auto_correct).pack(side=\"left\", padx=4)\n",
    "        ttk.Button(bar, text=\"Prev Issue\", command=lambda: self.jump_issue(-1)).pack(side=\"left\", padx=4)\n",
    "        ttk.Button(bar, text=\"Next Issue\", command=lambda: self.jump_issue(+1)).pack(side=\"left\", padx=4)\n",
    "        ttk.Button(bar, text=\"Corpus Word List\", command=self.open_vocab_browser).pack(side=\"left\", padx=4)\n",
    "        self.lbl_status = ttk.Label(bar, text=\"Ready.\"); self.lbl_status.pack(side=\"right\")\n",
    "\n",
    "        # Split view\n",
    "        split = ttk.Panedwindow(self, orient=\"horizontal\"); split.pack(fill=\"both\", expand=True, padx=10, pady=(0,10))\n",
    "\n",
    "        # Left: editor\n",
    "        left = ttk.Frame(split); split.add(left, weight=3)\n",
    "        self.txt = tk.Text(left, wrap=\"word\", height=20, undo=True,\n",
    "                           bg=\"#4E342E\", fg=\"#FFF3E0\", insertbackground=\"#FFF3E0\",\n",
    "                           selectbackground=\"#6D4C41\", font=(\"Menlo\", 12))\n",
    "        self.txt.pack(fill=\"both\", expand=True, side=\"left\")\n",
    "        sb = ttk.Scrollbar(left, command=self.txt.yview); sb.pack(side=\"right\", fill=\"y\")\n",
    "        self.txt.configure(yscrollcommand=sb.set)\n",
    "\n",
    "        # Char counter\n",
    "        meta = ttk.Frame(self); meta.pack(fill=\"x\", padx=10, pady=(0,10))\n",
    "        self.lbl_count = ttk.Label(meta, text=f\"0/{MAX_CHARS} chars\")\n",
    "        self.lbl_count.pack(side=\"right\")\n",
    "\n",
    "        # Right: Inspector\n",
    "        right = ttk.Frame(split); split.add(right, weight=2)\n",
    "        ttk.Label(right, text=\"Suggestion Inspector\", font=(\"TkDefaultFont\", 11, \"bold\")).pack(anchor=\"w\", padx=4, pady=(0,6))\n",
    "        cols = (\"Index\", \"Word\", \"Type\", \"Best\", \"Î”Score\", \"Edit\")\n",
    "        self.tree = ttk.Treeview(right, columns=cols, show=\"headings\", height=16)\n",
    "        for c, w in zip(cols, (60, 120, 100, 140, 80, 60)):\n",
    "            self.tree.heading(c, text=c)\n",
    "            self.tree.column(c, width=w, anchor=\"center\")\n",
    "        self.tree.pack(fill=\"both\", expand=True)\n",
    "        self.tree.bind(\"<<TreeviewSelect>>\", self._on_tree_select)\n",
    "\n",
    "        # Suggestions detail + actions\n",
    "        act = ttk.Frame(right); act.pack(fill=\"x\", pady=6)\n",
    "        self.btn_apply_best = ttk.Button(act, text=\"Replace Best\", command=self.replace_best, state=\"disabled\")\n",
    "        self.btn_apply_best.pack(side=\"left\", padx=4)\n",
    "        self.btn_apply_sel  = ttk.Button(act, text=\"Replace Selectedâ€¦\", command=self.replace_selected, state=\"disabled\")\n",
    "        self.btn_apply_sel.pack(side=\"left\", padx=4)\n",
    "\n",
    "        ttk.Label(right, text=\"Candidates for selected word:\").pack(anchor=\"w\", padx=4, pady=(6,2))\n",
    "        s_cols = (\"Candidate\", \"Score\", \"Edit\")\n",
    "        self.tree_sug = ttk.Treeview(right, columns=s_cols, show=\"headings\", height=6)\n",
    "        for c, w in zip(s_cols, (160, 100, 60)):\n",
    "            self.tree_sug.heading(c, text=c)\n",
    "            self.tree_sug.column(c, width=w, anchor=\"center\")\n",
    "        self.tree_sug.pack(fill=\"x\", padx=0, pady=(0,6))\n",
    "\n",
    "        # highlight tag styles\n",
    "        self.txt.tag_config(\"miss_nonword\", underline=True, foreground=\"#FF7043\")\n",
    "        self.txt.tag_config(\"miss_real\", underline=True, foreground=\"#FFB74D\")\n",
    "        self.txt.tag_config(\"focus_word\", background=\"#6D4C41\")\n",
    "\n",
    "        # bind events\n",
    "        self.txt.bind(\"<KeyRelease>\", lambda e: (self._enforce_limit(), self._clear_highlights()))\n",
    "        self.txt.bind(\"<Button-1>\", self._click_in_text)\n",
    "\n",
    "    def _bind_keys(self):\n",
    "        self.bind_all(\"<Control-Return>\", lambda e: self.run_check())\n",
    "        self.bind_all(\"<Command-Return>\", lambda e: self.run_check())\n",
    "\n",
    "    # ---------- logic ----------\n",
    "    def _enforce_limit(self):\n",
    "        content = self.txt.get(\"1.0\", \"end-1c\")\n",
    "        if len(content) > MAX_CHARS:\n",
    "            self.txt.delete(\"1.0\", \"end\")\n",
    "            self.txt.insert(\"1.0\", content[:MAX_CHARS])\n",
    "            content = self.txt.get(\"1.0\", \"end-1c\")\n",
    "        self.lbl_count.config(text=f\"{len(content)}/{MAX_CHARS} chars\")\n",
    "\n",
    "    def _clear_highlights(self):\n",
    "        for tag in self.txt.tag_names():\n",
    "            if tag.startswith(\"miss_\") or tag == \"focus_word\":\n",
    "                self.txt.tag_remove(tag, \"1.0\", \"end\")\n",
    "\n",
    "    def _highlight_results(self):\n",
    "        self._clear_highlights()\n",
    "        flagged = {r[\"index\"]: r for r in self.detect_results}\n",
    "        self.issue_indices = list(flagged.keys())\n",
    "        for i, (_, s, e) in enumerate(self.spans):\n",
    "            if i in flagged:\n",
    "                kind = flagged[i][\"type\"]\n",
    "                tag = \"miss_nonword\" if kind == \"NON-WORD\" else \"miss_real\"\n",
    "                self.txt.tag_add(tag, f\"1.0+{s}c\", f\"1.0+{e}c\")\n",
    "\n",
    "    def run_check(self):\n",
    "        content = self.txt.get(\"1.0\", \"end-1c\")\n",
    "        self.spans = [(t, s, e) for t, s, e in tokenize_with_spans(content)]\n",
    "        self.tokens = [t for t, _, _ in self.spans]\n",
    "        try:\n",
    "            # ðŸ”§ fixed: call detect_and_suggest_hybrid\n",
    "            toks, results = detect_and_suggest_hybrid(content, topk=5, ctx_margin=1.8)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"detect_and_suggest failed:\\n{e}\")\n",
    "            return\n",
    "        self.detect_results = results or []\n",
    "        self._highlight_results()\n",
    "        self._refresh_inspector()\n",
    "        n = len(self.detect_results)\n",
    "        self.lbl_status.config(text=(\"No issues found âœ…\" if n == 0 else f\"Found {n} potential issue(s).\"))\n",
    "\n",
    "    def _refresh_inspector(self):\n",
    "        self.tree.delete(*self.tree.get_children())\n",
    "        for r in self.detect_results:\n",
    "            idx = r[\"index\"]\n",
    "            word = r[\"word\"]\n",
    "            typ = r[\"type\"]\n",
    "            best = r[\"suggestions\"][0][\"cand\"] if r.get(\"suggestions\") else \"-\"\n",
    "            delta = r.get(\"best_delta\", \"\")\n",
    "            editd = r[\"suggestions\"][0][\"edit\"] if r.get(\"suggestions\") else \"\"\n",
    "            self.tree.insert(\"\", \"end\", iid=str(idx), values=(idx, word, typ, best, delta, editd))\n",
    "        self.tree_sug.delete(*self.tree_sug.get_children())\n",
    "        self.btn_apply_best.config(state=\"disabled\")\n",
    "        self.btn_apply_sel.config(state=\"disabled\")\n",
    "\n",
    "    def _on_tree_select(self, event=None):\n",
    "        sel = self.tree.selection()\n",
    "        self.tree_sug.delete(*self.tree_sug.get_children())\n",
    "        self._clear_focus_word()\n",
    "        if not sel:\n",
    "            self.btn_apply_best.config(state=\"disabled\")\n",
    "            self.btn_apply_sel.config(state=\"disabled\")\n",
    "            return\n",
    "        idx = int(sel[0])\n",
    "        self._focus_token(idx)\n",
    "        rec = next((r for r in self.detect_results if r[\"index\"] == idx), None)\n",
    "        if not rec:\n",
    "            self.btn_apply_best.config(state=\"disabled\")\n",
    "            self.btn_apply_sel.config(state=\"disabled\")\n",
    "            return\n",
    "        for s in rec.get(\"suggestions\", []):\n",
    "            self.tree_sug.insert(\"\", \"end\", values=(s[\"cand\"], s[\"score\"], s[\"edit\"]))\n",
    "        state = \"normal\" if rec.get(\"suggestions\") else \"disabled\"\n",
    "        self.btn_apply_best.config(state=state)\n",
    "        self.btn_apply_sel.config(state=state)\n",
    "\n",
    "    def _focus_token(self, idx):\n",
    "        if 0 <= idx < len(self.spans):\n",
    "            _, s, e = self.spans[idx]\n",
    "            self.txt.tag_add(\"focus_word\", f\"1.0+{s}c\", f\"1.0+{e}c\")\n",
    "            self.txt.see(f\"1.0+{s}c\")\n",
    "\n",
    "    def _clear_focus_word(self):\n",
    "        self.txt.tag_remove(\"focus_word\", \"1.0\", \"end\")\n",
    "\n",
    "    def _click_in_text(self, event):\n",
    "        index = self.txt.index(f\"@{event.x},{event.y}\")\n",
    "        upto = self.txt.get(\"1.0\", index)\n",
    "        pos = len(upto)\n",
    "        clicked = None\n",
    "        for i, (_, s, e) in enumerate(self.spans):\n",
    "            if s <= pos <= e:\n",
    "                clicked = i; break\n",
    "        if clicked is None:\n",
    "            return\n",
    "        if str(clicked) in self.tree.get_children():\n",
    "            self.tree.selection_set(str(clicked))\n",
    "            self.tree.focus(str(clicked))\n",
    "            self._on_tree_select()\n",
    "\n",
    "    def replace_best(self):\n",
    "        sel = self.tree.selection()\n",
    "        if not sel: return\n",
    "        idx = int(sel[0])\n",
    "        rec = next((r for r in self.detect_results if r[\"index\"] == idx), None)\n",
    "        if not rec or not rec.get(\"suggestions\"): return\n",
    "        best = rec[\"suggestions\"][0][\"cand\"]\n",
    "        self._replace_token(idx, best)\n",
    "\n",
    "    def replace_selected(self):\n",
    "        sel = self.tree.selection()\n",
    "        sel2 = self.tree_sug.selection()\n",
    "        if not sel or not sel2: return\n",
    "        idx = int(sel[0])\n",
    "        cand = self.tree_sug.item(sel2[0])[\"values\"][0]\n",
    "        self._replace_token(idx, cand)\n",
    "\n",
    "    def _replace_token(self, idx, cand):\n",
    "        if not (0 <= idx < len(self.spans)): return\n",
    "        _, s, e = self.spans[idx]\n",
    "        self.txt.delete(f\"1.0+{s}c\", f\"1.0+{e}c\")\n",
    "        self.txt.insert(f\"1.0+{s}c\", cand)\n",
    "        self.run_check()\n",
    "\n",
    "    def auto_correct(self):\n",
    "        content = self.txt.get(\"1.0\", \"end-1c\")\n",
    "        try:\n",
    "            # ðŸ”§ fixed: call detect_and_suggest_hybrid\n",
    "            toks, results = detect_and_suggest_hybrid(content, topk=5, ctx_margin=1.8)\n",
    "            corrected = apply_corrections(content, results)\n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Error\", f\"auto-correct failed:\\n{e}\")\n",
    "            return\n",
    "        self.txt.delete(\"1.0\", \"end\")\n",
    "        self.txt.insert(\"1.0\", corrected)\n",
    "        self.run_check()\n",
    "\n",
    "    def jump_issue(self, step):\n",
    "        if not self.detect_results: return\n",
    "        self.issue_cursor = (self.issue_cursor + step) % len(self.detect_results)\n",
    "        idx = self.detect_results[self.issue_cursor][\"index\"]\n",
    "        if str(idx) in self.tree.get_children():\n",
    "            self.tree.selection_set(str(idx))\n",
    "            self.tree.focus(str(idx))\n",
    "            self._on_tree_select()\n",
    "\n",
    "    def open_vocab_browser(self):\n",
    "        vb = tk.Toplevel(self)\n",
    "        vb.title(\"Corpus Vocabulary â€” Hybrid\")\n",
    "        vb.geometry(\"560x520\")\n",
    "        vb.configure(bg=\"#3E2723\")\n",
    "        ttk.Label(vb, text=\"Search vocabulary (Hybrid Corpus):\", font=(\"TkDefaultFont\", 10, \"bold\")).pack(anchor=\"w\", padx=10, pady=(10,4))\n",
    "\n",
    "        frm = ttk.Frame(vb); frm.pack(fill=\"x\", padx=10, pady=6)\n",
    "        qvar = tk.StringVar()\n",
    "        entry = ttk.Entry(frm, textvariable=qvar, width=30); entry.pack(side=\"left\", padx=(0,6))\n",
    "        entry.configure(foreground=\"black\", background=\"white\")\n",
    "        def refresh():\n",
    "            lb.delete(*lb.get_children())\n",
    "            q = (qvar.get() or \"\").lower().strip()\n",
    "            try:\n",
    "                items = search_vocab_hybrid(q, top=600) if q else sorted_vocab_hybrid[:600]\n",
    "            except Exception:\n",
    "                items = []\n",
    "            for w, c in items:\n",
    "                lb.insert(\"\", \"end\", values=(w, c))\n",
    "\n",
    "        ttk.Button(frm, text=\"Search\", command=refresh).pack(side=\"left\")\n",
    "        ttk.Button(frm, text=\"Top 600\", command=lambda:(qvar.set(\"\"), refresh())).pack(side=\"left\", padx=6)\n",
    "\n",
    "        cols = (\"Word\", \"Count\")\n",
    "        lb = ttk.Treeview(vb, columns=cols, show=\"headings\", height=18)\n",
    "        for c, w in zip(cols, (300, 120)):\n",
    "            lb.heading(c, text=c)\n",
    "            lb.column(c, width=w, anchor=\"w\")\n",
    "        lb.pack(fill=\"both\", expand=True, padx=10, pady=(0,10))\n",
    "\n",
    "        refresh()\n",
    "        entry.bind(\"<Return>\", lambda e: refresh())\n",
    "\n",
    "# --- Launch app ---\n",
    "if __name__ == \"__main__\":\n",
    "    app = SpellApp()\n",
    "    app.mainloop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
